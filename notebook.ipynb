{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Introduction to Deep Learning with PyTorch\n",
                "\n",
                "## Introduction to PyTorch, a Deep Learning Library\n",
                "\n",
                "### Creating tensors and accessing attributes\n",
                "\n",
                "Tensors are the primary data structure in PyTorch and will be the\n",
                "building blocks for our deep learning models. They share many\n",
                "similarities with NumPy arrays but have some unique attributes too.\n",
                "\n",
                "In this exercise, you'll practice creating a tensor from a Python list\n",
                "and displaying some of its attributes.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Begin by importing PyTorch.\n",
                "- Create a tensor from the Python list `list_a`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Defaulting to user installation because normal site-packages is not writeable\n",
                        "Requirement already satisfied: torch in /home/vscode/.local/lib/python3.12/site-packages (2.3.0)\n",
                        "Requirement already satisfied: filelock in /home/vscode/.local/lib/python3.12/site-packages (from torch) (3.14.0)\n",
                        "Requirement already satisfied: typing-extensions>=4.8.0 in /home/vscode/.local/lib/python3.12/site-packages (from torch) (4.12.0)\n",
                        "Requirement already satisfied: sympy in /home/vscode/.local/lib/python3.12/site-packages (from torch) (1.12)\n",
                        "Requirement already satisfied: networkx in /home/vscode/.local/lib/python3.12/site-packages (from torch) (3.3)\n",
                        "Requirement already satisfied: jinja2 in /home/vscode/.local/lib/python3.12/site-packages (from torch) (3.1.4)\n",
                        "Requirement already satisfied: fsspec in /home/vscode/.local/lib/python3.12/site-packages (from torch) (2024.5.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /home/vscode/.local/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
                        "Requirement already satisfied: mpmath>=0.19 in /home/vscode/.local/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "pip install torch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "cpu\n",
                        "torch.int64\n"
                    ]
                }
            ],
            "source": [
                "# Import PyTorch\n",
                "import torch\n",
                "\n",
                "list_a = [1, 2, 3, 4]\n",
                "\n",
                "# Create a tensor from list_a\n",
                "tensor_a = torch.tensor(list_a)\n",
                "\n",
                "# Display the tensor device\n",
                "print(tensor_a.device)\n",
                "\n",
                "# Display the tensor data type\n",
                "print(tensor_a.dtype)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Creating tensors from NumPy arrays\n",
                "\n",
                "Tensors are the fundamental data structure of PyTorch. You can create\n",
                "complex deep learning algorithms by learning how to manipulate them.\n",
                "\n",
                "The `torch` package has been imported, and two NumPy arrays have been\n",
                "created, named `array_a` and `array_b`. Both arrays have the same\n",
                "dimensions.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create two tensors, `tensor_a` and `tensor_b`, from the NumPy arrays\n",
                "  `array_a` and `array_b`, respectively.\n",
                "- Subtract `tensor_b` from `tensor_a`.\n",
                "- Perform an element-wise multiplication of `tensor_a` and `tensor_b`.\n",
                "- Add the resulting tensors from the two previous steps together.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Defaulting to user installation because normal site-packages is not writeable\n",
                        "Requirement already satisfied: numpy in /home/vscode/.local/lib/python3.12/site-packages (1.26.4)\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "pip install numpy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# added/edited\n",
                "import numpy as np\n",
                "array_a = np.array([[1, 1, 1], [2, 3, 4], [4, 5, 6]])\n",
                "array_b = np.array([[7, 5, 4], [2, 2, 8], [6, 3, 8]])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([[ 1,  1,  1],\n",
                        "        [ 4,  7, 28],\n",
                        "        [22, 17, 46]])\n"
                    ]
                }
            ],
            "source": [
                "# Create two tensors from the arrays\n",
                "tensor_a = torch.from_numpy(array_a)\n",
                "tensor_b = torch.from_numpy(array_b)\n",
                "\n",
                "# Subtract tensor_b from tensor_a \n",
                "tensor_c = tensor_a - tensor_b\n",
                "\n",
                "# Multiply each element of tensor_a with each element of tensor_b\n",
                "tensor_d = tensor_a * tensor_b\n",
                "\n",
                "# Add tensor_c with tensor_d\n",
                "tensor_e = tensor_c + tensor_d\n",
                "print(tensor_e)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Your first neural network\n",
                "\n",
                "In this exercise, you will implement a small neural network containing\n",
                "two **linear** layers. The first layer takes an eight-dimensional input,\n",
                "and the last layer outputs a one-dimensional tensor.\n",
                "\n",
                "The `torch` package and the `torch.nn` package have already been\n",
                "imported for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create a neural network of linear layers that takes a tensor of\n",
                "  dimensions $1\\times8$ as input and outputs a tensor of dimensions\n",
                "  $1\\times1$.\n",
                "- Use any output dimension for the first layer you want.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([[0.8785]], grad_fn=<AddmmBackward0>)\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "input_tensor = torch.Tensor([[2, 3, 6, 7, 9, 3, 2, 1]])\n",
                "\n",
                "# Implement a small neural network with exactly two linear layers\n",
                "model = nn.Sequential(nn.Linear(8, 4),\n",
                "                      nn.Linear(4, 1),\n",
                "                     )\n",
                "\n",
                "output = model(input_tensor)\n",
                "print(output)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### The sigmoid and softmax functions\n",
                "\n",
                "The sigmoid and softmax functions are two of the most popular activation\n",
                "functions in deep learning. They are both usually used as the last step\n",
                "of a neural network. Sigmoid functions are used for binary\n",
                "classification problems, whereas softmax functions are often used for\n",
                "multi-class classification problems. This exercise will familiarize you\n",
                "with creating and using both functions.\n",
                "\n",
                "Let's say that you have a neural network that returned the values\n",
                "contained in the `score` tensor as a pre-activation output. You will\n",
                "apply activation functions to this output.\n",
                "\n",
                "`torch.nn` is already imported as `nn`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "Create a sigmoid function and apply it on `input_tensor` to generate a\n",
                "probability.\n",
                "\n",
                "Create a softmax function and apply it on `input_tensor` to generate a\n",
                "probability.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([[0.6900]])\n",
                        "tensor([[1.2828e-01, 1.1698e-04, 5.7492e-01, 3.4961e-02, 1.5669e-01, 1.0503e-01]])\n"
                    ]
                }
            ],
            "source": [
                "input_tensor = torch.tensor([[0.8]])\n",
                "\n",
                "# Create a sigmoid function and apply it on input_tensor\n",
                "sigmoid = nn.Sigmoid()\n",
                "probability = sigmoid(input_tensor)\n",
                "print(probability)\n",
                "\n",
                "input_tensor = torch.tensor([[1.0, -6.0, 2.5, -0.3, 1.2, 0.8]])\n",
                "\n",
                "# Create a softmax function and apply it on input_tensor\n",
                "softmax = nn.Softmax(dim=-1)\n",
                "probabilities = softmax(input_tensor)\n",
                "print(probabilities)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Our First Neural Network with PyTorch\n",
                "\n",
                "### Building a binary classifier in PyTorch\n",
                "\n",
                "Recall that a small neural network with a single linear layer followed\n",
                "by a sigmoid function is a binary classifier. It acts just like a\n",
                "logistic regression.\n",
                "\n",
                "In this exercise, you'll practice building this small network and\n",
                "interpreting the output of the classifier.\n",
                "\n",
                "The `torch` package and the `torch.nn` package have already been\n",
                "imported for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create a neural network that takes a tensor of dimensions 1x8 as\n",
                "  input, and returns an output of the correct shape for binary\n",
                "  classification.\n",
                "- Pass the output of the linear layer to a sigmoid, which both takes in\n",
                "  and return a single float.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([[0.1250]], grad_fn=<SigmoidBackward0>)\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
                "\n",
                "# Implement a small neural network for binary classification\n",
                "model = nn.Sequential(\n",
                "  nn.Linear(8, 1),\n",
                "  nn.Sigmoid()\n",
                ")\n",
                "\n",
                "output = model(input_tensor)\n",
                "print(output)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### From regression to multi-class classification\n",
                "\n",
                "Recall that the models we have seen for binary classification,\n",
                "multi-class classification and regression have all been similar, barring\n",
                "a few tweaks to the model.\n",
                "\n",
                "In this exercise, you'll start by building a model for regression, and\n",
                "then tweak the model to perform a multi-class classification.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create a neural network with exactly four linear layers, which takes\n",
                "  the input tensor as input, and outputs a regression value, using any\n",
                "  shapes you like for the hidden layers.\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "- A similar neural network to the one you just built is provided,\n",
                "  containing four linear layers; update this network to perform a\n",
                "  multi-class classification with four outputs.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([[-0.0369]], grad_fn=<AddmmBackward0>)\n",
                        "tensor([[ 1.1519,  0.0199, -0.1981,  0.5716]], grad_fn=<AddmmBackward0>)\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
                "\n",
                "# Implement a neural network with exactly four linear layers\n",
                "model = nn.Sequential(\n",
                "  nn.Linear(11, 20),\n",
                "  nn.Linear(20, 12),\n",
                "  nn.Linear(12, 6),\n",
                "  nn.Linear(6, 1),  \n",
                ")\n",
                "\n",
                "output = model(input_tensor)\n",
                "print(output)\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
                "\n",
                "# Update network below to perform a multi-class classification with four labels\n",
                "model = nn.Sequential(\n",
                "  nn.Linear(11, 20),\n",
                "  nn.Linear(20, 12),\n",
                "  nn.Linear(12, 6),\n",
                "  nn.Linear(6, 4), \n",
                ")\n",
                "\n",
                "output = model(input_tensor)\n",
                "print(output)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Creating one-hot encoded labels\n",
                "\n",
                "One-hot encoding is a technique that turns a single integer label into a\n",
                "vector of N elements, where N is the number of classes in your dataset.\n",
                "This vector only contains zeros and ones. In this exercise, you'll\n",
                "create the one-hot encoded vector of the label `y` provided.\n",
                "\n",
                "You'll practice doing this manually, and then make your life easier by\n",
                "leveraging the help of PyTorch! Your dataset contains three classes.\n",
                "\n",
                "NumPy is already imported as `np`, and `torch.nn.functional` as `F`. The\n",
                "`torch` package is also imported.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Manually create a one-hot encoded vector of the ground truth label `y`\n",
                "  by filling in the NumPy array provided.\n",
                "- Create a one-hot encoded vector of the ground truth label `y` using\n",
                "  PyTorch.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "# added/edited\n",
                "import torch.nn.functional as F"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "y = 1\n",
                "num_classes = 3\n",
                "\n",
                "# Create the one-hot encoded vector using NumPy\n",
                "one_hot_numpy = np.array([0, 1, 0])\n",
                "\n",
                "# Create the one-hot encoded vector using PyTorch\n",
                "one_hot_pytorch = F.one_hot(torch.tensor(y), num_classes)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Calculating cross entropy loss\n",
                "\n",
                "Cross entropy loss is the most used loss for classification problems. In\n",
                "this exercise, you will create inputs and calculate cross entropy loss\n",
                "in PyTorch. You are provided with the ground truth label `y` and a\n",
                "vector of `scores` predicted by your model.\n",
                "\n",
                "You'll start by creating a one-hot encoded vector of the ground truth\n",
                "label `y`, which is a required step to compare `y` with the scores\n",
                "predicted by your model. Next, you'll create a cross entropy loss\n",
                "function. Last, you'll call the loss function, which takes `scores`\n",
                "(model predictions before the final softmax function), and the one-hot\n",
                "encoded ground truth label, as inputs. It outputs a single float, the\n",
                "loss of that sample.\n",
                "\n",
                "`torch`, `torch.nn` as `nn`, and `torch.nn.functional` as `F` have\n",
                "already been imported for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create the one-hot encoded vector of the ground truth label `y` and\n",
                "  assign it to `one_hot_label`.\n",
                "- Create the cross entropy loss function and store it as `criterion`.\n",
                "- Calculate the cross entropy loss using the `one_hot_label` vector and the `scores` vector, by calling the `loss_function` you created.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor(8.0619, dtype=torch.float64)\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "y = [2]\n",
                "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
                "\n",
                "# Create a one-hot encoded vector of the label y\n",
                "one_hot_label = F.one_hot(torch.tensor(y), num_classes=scores.shape[1])\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "y = [2]\n",
                "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
                "\n",
                "# Create a one-hot encoded vector of the label y\n",
                "one_hot_label = F.one_hot(torch.tensor(y), num_classes = scores.shape[1])\n",
                "\n",
                "# Create the cross entropy loss function\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "y = [2]\n",
                "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
                "\n",
                "# Create a one-hot encoded vector of the label y\n",
                "one_hot_label = F.one_hot(torch.tensor(y), scores.shape[1])\n",
                "\n",
                "# Create the cross entropy loss function\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "\n",
                "# Calculate the cross entropy loss\n",
                "loss = criterion(scores.double(), one_hot_label.double())\n",
                "print(loss)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Estimating a sample\n",
                "\n",
                "In previous exercises, you used linear layers to build networks.\n",
                "\n",
                "Recall that the operation performed by `nn.Linear()` is to take an input\n",
                "$X$ and apply the transformation $W*X + b$ ,where $W$ and $b$\n",
                "are two tensors (called the weight and bias).\n",
                "\n",
                "A critical part of training PyTorch models is to calculate gradients of\n",
                "the weight and bias tensors with respect to a loss function.\n",
                "\n",
                "In this exercise, you will calculate weight and bias tensor gradients\n",
                "using cross entropy loss and a sample of data.\n",
                "\n",
                "The following tensors are provded:\n",
                "\n",
                "- `weight`: a $2 \\times 9$-element tensor\n",
                "- `bias`: a $2$-element tensor\n",
                "- `preds`: a $1 \\times 2$-element tensor containing the model\n",
                "  predictions\n",
                "- `target`: a $1 \\times 2$-element one-hot encoded tensor containing\n",
                "  the ground-truth label\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Use the criterion you have defined to calculate the loss value with\n",
                "  respect to the predictions and target values.\n",
                "- Compute the gradients of the cross entropy loss.\n",
                "- Display the gradients of the weight and bias tensors, in that order.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "# added/edited\n",
                "tensor1 = torch.tensor([[-0.2785, 0.0695]], requires_grad=True)\n",
                "tensor2 = torch.zeros_like(tensor1, requires_grad=True)\n",
                "preds = tensor1 + tensor2\n",
                "target = torch.tensor([[1., 0.]])\n",
                "weight = torch.tensor([[-1.0375, -0.1008, -0.1284,  1.3466, -0.4017, -0.0807,  0.1226, -1.8221, 0.9174], [ 0.4857, -0.8295,  1.2543, -0.1574,  1.2001,  0.7425, -1.1563, -0.1283, 0.0028]], requires_grad=True)\n",
                "bias = torch.tensor([ 0.2775, -0.4224], requires_grad=True)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "None\n",
                        "None\n"
                    ]
                }
            ],
            "source": [
                "criterion = nn.CrossEntropyLoss()\n",
                "\n",
                "# Calculate the loss\n",
                "loss = criterion(preds, target)\n",
                "\n",
                "# Compute the gradients of the loss\n",
                "loss.backward()\n",
                "\n",
                "# Display gradients of the weight and bias tensors in order\n",
                "print(weight.grad)\n",
                "print(bias.grad)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Accessing the model parameters\n",
                "\n",
                "A PyTorch model created with the `nn.Sequential()` is a module that\n",
                "contains the different layers of your network. Recall that each layer\n",
                "parameter can be accessed by indexing the created model directly. In\n",
                "this exercise, you will practice accessing the parameters of different\n",
                "**linear** layers of a neural network. You won't be accessing the\n",
                "sigmoid.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Access the `weight` parameter of the first **linear** layer.\n",
                "- Access the `bias` parameter of the second **linear** layer.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = nn.Sequential(nn.Linear(16, 8),\n",
                "                      nn.Sigmoid(),\n",
                "                      nn.Linear(8, 2))\n",
                "\n",
                "# Access the weight of the first linear layer\n",
                "weight_0 = model[0].weight\n",
                "\n",
                "# Access the bias of the second linear layer\n",
                "bias_1 = model[2].bias\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Updating the weights manually\n",
                "\n",
                "Now that you know how to access weights and biases, you will manually\n",
                "perform the job of the PyTorch optimizer. PyTorch functions can do what\n",
                "you're about to do, but it's helpful to do the work manually at least\n",
                "once, to understand what's going on under the hood.\n",
                "\n",
                "A neural network of three layers has been created and stored as the\n",
                "`model` variable. This network has been used for a forward pass and the\n",
                "loss and its derivatives have been calculated. A default learning rate,\n",
                "`lr`, has been chosen to scale the gradients when performing the update.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create the gradient variables by accessing the local gradients of each\n",
                "  weight tensor.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "weight0 = model[0].weight\n",
                "weight1 = model[1].weight\n",
                "weight2 = model[2].weight\n",
                "\n",
                "# Access the gradients of the weight of each linear layer\n",
                "grads0 = weight0.grad\n",
                "grads1 = weight1.grad\n",
                "grads2 = weight2.grad\n",
                "\n",
                "weight0 = model[0].weight\n",
                "weight1 = model[1].weight\n",
                "weight2 = model[2].weight\n",
                "\n",
                "# Access the gradients of the weight of each linear layer\n",
                "grads0 = weight0.grad\n",
                "grads1 = weight1.grad\n",
                "grads2 = weight2.grad\n",
                "\n",
                "# Update the weights using the learning rate and the gradients\n",
                "weight0 = weight0 - lr * grads0\n",
                "weight1 = weight1 - lr * grads1\n",
                "weight2 = weight2 - lr * grads2\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Using the PyTorch optimizer\n",
                "\n",
                "In the previous exercise, you manually updated the weight of a network.\n",
                "You now know what's going on under the hood, but this approach is not\n",
                "scalable to a network of many layers.\n",
                "\n",
                "Thankfully, the PyTorch SGD optimizer does a similar job in a handful of\n",
                "lines of code. In this exercise, you will practice the last step to\n",
                "complete the training loop: updating the weights using a PyTorch\n",
                "optimizer.\n",
                "\n",
                "A neural network has been created and provided as the `model` variable.\n",
                "This model was used to run a forward pass and create the tensor of\n",
                "predictions `pred`. The one-hot encoded tensor is named `target` and the\n",
                "cross entropy loss function is stored as `criterion`.\n",
                "\n",
                "`torch.optim` as `optim`, and `torch.nn` as `nn` have already been\n",
                "loaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Use `optim` to create an SGD optimizer with a learning rate of your\n",
                "  choice (must be less than one) for the `model` provided.\n",
                "- Update the model's parameters using the optimizer.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the optimizer\n",
                "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
                "\n",
                "# Create the optimizer\n",
                "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
                "\n",
                "loss = criterion(pred, target)\n",
                "loss.backward()\n",
                "\n",
                "# Update the model's parameters using the optimizer\n",
                "optimizer.step()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Using the MSELoss\n",
                "\n",
                "Recall that we can't use cross-entropy loss for regression problems. The\n",
                "mean squared error loss (MSELoss) is a common loss function for\n",
                "regression problems. In this exercise, you will practice calculating and\n",
                "observing the loss using NumPy as well as its PyTorch implementation.\n",
                "\n",
                "The `torch` package has been imported as well as `numpy` as `np` and\n",
                "`torch.nn` as `nn`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Calculate the MSELoss using NumPy.\n",
                "- Create a MSELoss function using PyTorch.\n",
                "- Convert `y_hat` and `y` to tensors and then float data types, and then\n",
                "  use them to calculate MSELoss using PyTorch as `mse_pytorch`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_hat = np.array(10)\n",
                "y = np.array(1)\n",
                "\n",
                "# Calculate the MSELoss using NumPy\n",
                "mse_numpy = np.mean((y_hat - y)**2)\n",
                "\n",
                "# Create the MSELoss function\n",
                "criterion = nn.MSELoss()\n",
                "\n",
                "# Calculate the MSELoss using the created loss function\n",
                "mse_pytorch = criterion(torch.tensor(y_hat).float(), torch.tensor(y).float())\n",
                "print(mse_pytorch)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Writing a training loop\n",
                "\n",
                "In `scikit-learn`, the whole training loop is contained in the `.fit()`\n",
                "method. In PyTorch, however, you implement the loop manually. While this\n",
                "provides control over loop's content, it requires a custom\n",
                "implementation.\n",
                "\n",
                "You will write a training loop every time you train a deep learning\n",
                "model with PyTorch, which you'll practice in this exercise. The\n",
                "`show_results()` function provided will display some sample ground truth\n",
                "and the model predictions.\n",
                "\n",
                "The package imports provided are: pandas as `pd`, `torch`, `torch.nn` as\n",
                "`nn`, `torch.optim` as `optim`, as well as `DataLoader` and\n",
                "`TensorDataset` from `torch.utils.data`.\n",
                "\n",
                "The following variables have been created: `dataloader`, containing the\n",
                "dataloader; `model`, containing the neural network; `criterion`,\n",
                "containing the loss function, `nn.MSELoss()`; `optimizer`, containing\n",
                "the SGD optimizer; and `num_epochs`, containing the number of epochs.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Write a for loop that iterates over the `dataloader`; this should be\n",
                "  nested within a for loop that iterates over a range equal to the\n",
                "  number of epochs.\n",
                "- Set the gradients of the optimizer to zero.\n",
                "- Write the forward pass.\n",
                "- Compute the MSE loss value using the criterion() function provided.\n",
                "- Compute the gradients.\n",
                "- Update the model's parameters.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Loop over the number of epochs and then the dataloader\n",
                "for i in range(num_epochs):\n",
                "  for data in dataloader:\n",
                "    # Set the gradients to zero\n",
                "    optimizer.zero_grad()\n",
                "\n",
                "# Loop over the number of epochs and the dataloader\n",
                "for i in range(num_epochs):\n",
                "  for data in dataloader:\n",
                "    # Set the gradients to zero\n",
                "    optimizer.zero_grad()\n",
                "    # Run a forward pass\n",
                "    feature, target = data\n",
                "    prediction = model(feature)    \n",
                "    # Calculate the loss\n",
                "    loss = criterion(prediction, target)    \n",
                "    # Compute the gradients\n",
                "    loss.backward()\n",
                "\n",
                "# Loop over the number of epochs and the dataloader\n",
                "for i in range(num_epochs):\n",
                "  for data in dataloader:\n",
                "    # Set the gradients to zero\n",
                "    optimizer.zero_grad()  \n",
                "    # Run a forward pass\n",
                "    feature, target = data\n",
                "    prediction = model(feature)    \n",
                "    # Calculate the loss\n",
                "    loss = criterion(prediction, target)    \n",
                "    # Compute the gradients\n",
                "    loss.backward()\n",
                "    # Update the model's parameters\n",
                "    optimizer.step()\n",
                "show_results(model, dataloader)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Neural Network Architecture and Hyperparameters\n",
                "\n",
                "### Implementing ReLU\n",
                "\n",
                "The rectified linear unit (or ReLU) function is one of the most common\n",
                "activation functions in deep learning.\n",
                "\n",
                "It overcomes the training problems linked with the sigmoid function you\n",
                "learned, such as the **vanishing gradients problem**.\n",
                "\n",
                "In this exercise, you'll begin with a ReLU implementation in PyTorch.\n",
                "Next, you'll calculate the gradients of the function.\n",
                "\n",
                "The `nn` module has already been imported for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create a ReLU function in PyTorch.\n",
                "- Calculate the gradient of the ReLU function for `x` using the `relu_pytorch()` function you defined, then running a backward pass\n",
                "- Find the gradient at `x`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a ReLU function with PyTorch\n",
                "relu_pytorch = nn.ReLU()\n",
                "\n",
                "# Create a ReLU function with PyTorch\n",
                "relu_pytorch = nn.ReLU()\n",
                "\n",
                "# Apply your ReLU function on x, and calculate gradients\n",
                "x = torch.tensor(-1.0, requires_grad=True)\n",
                "y = relu_pytorch(x)\n",
                "y.backward()\n",
                "\n",
                "# Print the gradient of the ReLU function for x\n",
                "gradient = x.grad\n",
                "print(gradient)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Implementing leaky ReLU\n",
                "\n",
                "You've learned that ReLU is one of the most used activation functions in\n",
                "deep learning. You will find it in modern architecture. However, it does\n",
                "have the inconvenience of outputting null values for negative inputs and\n",
                "therefore, having null gradients. Once an element of the input is\n",
                "negative, it will be set to zero for the rest of the training. Leaky\n",
                "ReLU overcomes this challenge by using a multiplying factor for negative\n",
                "inputs.\n",
                "\n",
                "In this exercise, you will implement the leaky ReLU function in NumPy\n",
                "and PyTorch and practice using it. The `numpy` as `np` package, the\n",
                "`torch` package as well as the `torch.nn` as `nn` have already been\n",
                "imported.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create a leaky ReLU function in PyTorch with a negative slope of 0.05.\n",
                "- Call the function on the tensor `x`, which has already been defined\n",
                "  for you.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a leaky relu function in PyTorch\n",
                "leaky_relu_pytorch = nn.LeakyReLU(negative_slope=0.05)\n",
                "\n",
                "x = torch.tensor(-2.0)\n",
                "# Call the above function on the tensor x\n",
                "output = leaky_relu_pytorch(x)\n",
                "print(output)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Counting the number of parameters\n",
                "\n",
                "Deep learning models are famous for having a lot of parameters. Recent\n",
                "language models have billions of parameters. With more parameters comes\n",
                "more computational complexity and longer training times, and a deep\n",
                "learning practitioner must know how many parameters their model has.\n",
                "\n",
                "In this exercise, you will calculate the number of parameters in your\n",
                "model, first using PyTorch then manually.\n",
                "\n",
                "The `torch.nn` package has been imported as `nn`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Iterate through the model's parameters to update the total variable\n",
                "  with the total number of parameters in the model.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = nn.Sequential(nn.Linear(16, 4),\n",
                "                      nn.Linear(4, 2),\n",
                "                      nn.Linear(2, 1))\n",
                "\n",
                "total = 0\n",
                "\n",
                "# Calculate the number of parameters in the model\n",
                "for p in model.parameters():\n",
                "  total += p.numel()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Manipulating the capacity of a network\n",
                "\n",
                "In this exercise, you will practice creating neural networks with\n",
                "different capacities. The capacity of a network reflects the number of\n",
                "parameters in said network. To help you, a `calculate_capacity()`\n",
                "function has been implemented, as follows:\n",
                "\n",
                "    def calculate_capacity(model):\n",
                "      total = 0\n",
                "      for p in model.parameters():\n",
                "        total += p.numel()\n",
                "      return total\n",
                "\n",
                "This function returns the number of parameters in the your model.\n",
                "\n",
                "The dataset you are training this network on has `n_features` features\n",
                "and `n_classes` classes. The `torch.nn` package has been imported as\n",
                "`nn`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "Create a neural network with exactly three linear layers and less than\n",
                "120 parameters, which takes `n_features` as inputs and outputs\n",
                "`n_classes`.\n",
                "\n",
                "Create a neural network with exactly four linear layers and more than\n",
                "120 parameters, which takes `n_features` as inputs and outputs\n",
                "`n_classes`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "n_features = 8\n",
                "n_classes = 2\n",
                "\n",
                "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
                "\n",
                "# Create a neural network with less than 120 parameters\n",
                "model = nn.Sequential(nn.Linear(n_features, 8),\n",
                "                      nn.Linear(8, 4),\n",
                "                      nn.Linear(4, n_classes))\n",
                "output = model(input_tensor)\n",
                "\n",
                "print(calculate_capacity(model))\n",
                "\n",
                "n_features = 8\n",
                "n_classes = 2\n",
                "\n",
                "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
                "\n",
                "# Create a neural network with more than 120 parameters\n",
                "model = nn.Sequential(nn.Linear(n_features, 16),\n",
                "                      nn.Linear(16, 8),\n",
                "                      nn.Linear(8, 3), \n",
                "                      nn.Linear(3, n_classes))\n",
                "\n",
                "output = model(input_tensor)\n",
                "\n",
                "print(calculate_capacity(model))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Experimenting with learning rate\n",
                "\n",
                "In this exercise, your goal is to find the optimal learning rate such\n",
                "that the optimizer can find the minimum of the non-convex function\n",
                "$x^{4} + x^{3} - 5x^{2}$ in ten steps.\n",
                "\n",
                "You will experiment with three different learning rate values. For this\n",
                "problem, try learning rate values between 0.001 to 0.1.\n",
                "\n",
                "You are provided with the `optimize_and_plot()` function that takes the\n",
                "learning rate for the first argument. This function will run 10 steps of\n",
                "the SGD optimizer and display the results.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Try a small learning rate value such that the optimizer isn't able to\n",
                "  get past the first minimum on the right.\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "- Try a large learning rate value such that the optimizer skips past the\n",
                "  global minimum at -2.\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "- Based on the previous results, try a better learning rate value.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Try a first learning rate value\n",
                "lr0 = 0.005\n",
                "optimize_and_plot(lr=lr0)\n",
                "\n",
                "# Try a second learning rate value\n",
                "lr1 = 0.1\n",
                "optimize_and_plot(lr=lr1)\n",
                "\n",
                "# Try a third learning rate value\n",
                "lr2 = 0.09\n",
                "optimize_and_plot(lr=lr2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Experimenting with momentum\n",
                "\n",
                "In this exercise, your goal is to find the optimal momentum such that\n",
                "the optimizer can find the minimum of the following non-convex function\n",
                "$x^{4} + x^{3} - 5x^{2}$ in 20 steps. You will experiment with two\n",
                "different momentum values. For this problem, the learning rate is fixed\n",
                "at 0.01.\n",
                "\n",
                "You are provided with the `optimize_and_plot()` function that takes the\n",
                "learning rate for the first argument. This function will run 20 steps of\n",
                "the SGD optimizer and display the results.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Try a first value for the momentum such that the optimizer gets stuck\n",
                "  in the first minimum.\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "- Try a second value for the momentum such that the optimizer finds the\n",
                "  global optimum.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Try a first value for momentum\n",
                "mom0 = 0.85\n",
                "optimize_and_plot(momentum=mom0)\n",
                "\n",
                "# Try a second value for momentum\n",
                "mom1 = 0.95\n",
                "optimize_and_plot(momentum=mom1)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Freeze layers of a model\n",
                "\n",
                "You are about to fine-tune a model on a new task after loading\n",
                "pre-trained weights. The model contains three linear layers. However,\n",
                "because your dataset is small, you only want to train the last linear\n",
                "layer of this model and freeze the first two linear layers.\n",
                "\n",
                "The model has already been created and exists under the variable\n",
                "`model`. You will be using the `named_parameters` method of the model to\n",
                "list the parameters of the model. Each parameter is described by a name.\n",
                "This name is a string with the following naming convention: `x.name`\n",
                "where `x` is the index of the layer.\n",
                "\n",
                "Remember that a linear layer has two parameters: the `weight` and the\n",
                "`bias`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Use an `if` statement to determine if the parameter should be frozen\n",
                "  or not based on its name.\n",
                "- Freeze the parameters of the first two layers of this model.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for name, param in model.named_parameters():\n",
                "  \n",
                "    # Check if the parameters belong to the first layer\n",
                "    if name == '0.weight' or name == '0.bias':\n",
                "   \n",
                "        # Freeze the parameters\n",
                "        param.requires_grad = False\n",
                "        \n",
                "    # Check if the parameters belong to the second layer\n",
                "    if name == '1.weight' or name == '1.bias':\n",
                "      \n",
                "        # Freeze the parameters\n",
                "        param.requires_grad = False\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Layer initialization\n",
                "\n",
                "The initialization of the weights of a neural network has been the focus\n",
                "of researchers for many years. When training a network, the method used\n",
                "to initialize the weights has a direct impact on the final performance\n",
                "of the network.\n",
                "\n",
                "As a machine learning practitioner, you should be able to experiment\n",
                "with different initialization strategies. In this exercise, you are\n",
                "creating a small neural network made of two layers and you are deciding\n",
                "to initialize each layer's weights with the uniform method.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- For each layer (`layer0` and `layer1`), use the uniform initialization\n",
                "  method to initialize the weights.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "layer0 = nn.Linear(16, 32)\n",
                "layer1 = nn.Linear(32, 64)\n",
                "\n",
                "# Use uniform initialization for layer0 and layer1 weights\n",
                "nn.init.uniform_(layer0.weight)\n",
                "nn.init.uniform_(layer1.weight)\n",
                "\n",
                "model = nn.Sequential(layer0, layer1)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluating and Improving Models\n",
                "\n",
                "### Using the TensorDataset class\n",
                "\n",
                "In practice, loading your data into a PyTorch dataset will be one of the\n",
                "first steps you take in order to create and train a neural network with\n",
                "PyTorch.\n",
                "\n",
                "The `TensorDataset` class is very helpful when your dataset can be\n",
                "loaded directly as a NumPy array. Recall that `TensorDataset()` can take\n",
                "one or more NumPy arrays as input.\n",
                "\n",
                "In this exercise, you'll practice creating a PyTorch dataset using the\n",
                "TensorDataset class.\n",
                "\n",
                "`torch` and `numpy` have already been imported for you, along with the\n",
                "`TensorDataset` class.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Convert the NumPy arrays provided to PyTorch tensors.\n",
                "- Create a TensorDataset using the `torch_features` and the\n",
                "  `torch_target` tensors provided (in this order).\n",
                "- Return the last element of the dataset.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch\n",
                "from torch.utils.data import TensorDataset\n",
                "\n",
                "np_features = np.array(np.random.rand(12, 8))\n",
                "np_target = np.array(np.random.rand(12, 1))\n",
                "\n",
                "# Convert arrays to PyTorch tensors\n",
                "torch_features = torch.tensor(np_features)\n",
                "torch_target = torch.tensor(np_target)\n",
                "\n",
                "# Create a TensorDataset from two tensors\n",
                "dataset = TensorDataset(torch_features, torch_target)\n",
                "\n",
                "# Return the last element of this dataset\n",
                "print(dataset[-1])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### From data loading to running a forward pass\n",
                "\n",
                "In this exercise, you'll create a PyTorch `DataLoader` from a pandas\n",
                "DataFrame and call a model on this dataset. Specifically, you'll run a\n",
                "**forward pass** on a neural network. You'll continue working with fully\n",
                "connected neural networks, as you have done so far.\n",
                "\n",
                "You'll begin by subsetting a loaded DataFrame called `dataframe`,\n",
                "converting features and targets NumPy arrays, and converting to PyTorch\n",
                "tensors in order to create a PyTorch dataset.\n",
                "\n",
                "This dataset can be loaded into a PyTorch `DataLoader`, batched,\n",
                "shuffled, and used to run a forward pass on a custom fully connected\n",
                "neural network.\n",
                "\n",
                "NumPy as `np`, pandas as `pd`, `torch`, `TensorDataset()`, and\n",
                "`DataLoader()` have been imported for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Extract the features (`ph`, `Sulfate`, `Conductivity`,\n",
                "  `Organic_carbon`) and target (`Potability`) values and load them into\n",
                "  the appropriate tensors to represent features and targets.\n",
                "- Use both tensors to create a PyTorch dataset using the dataset class\n",
                "  that's quickest to use when tensors don't require any additional\n",
                "  preprocessing.\n",
                "- Create a PyTorch `DataLoader` from the created `TensorDataset`; this `DataLoader` should use a `batch_size` of two and `shuffle` the dataset.\n",
                "- Implement a small, fully connected neural network using exactly two linear layers and the `nn.Sequential()` API, where the final output size is 1.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the different columns into two PyTorch tensors\n",
                "features = torch.tensor(dataframe[['ph', 'Sulfate', 'Conductivity', 'Organic_carbon']].to_numpy()).float()\n",
                "target = torch.tensor(dataframe['Potability'].to_numpy()).float()\n",
                "\n",
                "# Create a dataset from the two generated tensors\n",
                "dataset = TensorDataset(features, target)\n",
                "\n",
                "# Load the different columns into two PyTorch tensors\n",
                "features = torch.tensor(dataframe[['ph', 'Sulfate', 'Conductivity', 'Organic_carbon']].to_numpy()).float()\n",
                "target = torch.tensor(dataframe['Potability'].to_numpy()).float()\n",
                "\n",
                "# Create a dataset from the two generated tensors\n",
                "dataset = TensorDataset(features, target)\n",
                "\n",
                "# Create a dataloader using the above dataset\n",
                "dataloader = DataLoader(dataset, shuffle=True, batch_size=2)\n",
                "x, y = next(iter(dataloader))\n",
                "\n",
                "# Load the different columns into two PyTorch tensors\n",
                "features = torch.tensor(dataframe[['ph', 'Sulfate', 'Conductivity', 'Organic_carbon']].to_numpy()).float()\n",
                "target = torch.tensor(dataframe['Potability'].to_numpy()).float()\n",
                "\n",
                "# Create a dataset from the two generated tensors\n",
                "dataset = TensorDataset(features, target)\n",
                "\n",
                "# Create a dataloader using the above dataset\n",
                "dataloader = DataLoader(dataset, shuffle=True, batch_size=2)\n",
                "x, y = next(iter(dataloader))\n",
                "\n",
                "# Create a model using the nn.Sequential API\n",
                "model = nn.Sequential(\n",
                "  nn.Linear(4, 16), \n",
                "  nn.Linear(16, 1)\n",
                ")\n",
                "\n",
                "output = model(features)\n",
                "print(output)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Writing the evaluation loop\n",
                "\n",
                "In this exercise, you will practice writing the evaluation loop. Recall\n",
                "that the evaluation loop is similar to the training loop, except that\n",
                "you will not perform the gradient calculation and the optimizer step.\n",
                "\n",
                "The `model` has already been defined for you, along with the object\n",
                "`validationloader`, which is a dataset.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Set the model to evaluation mode.\n",
                "- Sum the current batch loss to the `validation_loss` variable.\n",
                "- Calculate the mean loss value for the epoch.\n",
                "- Set the model back to training mode.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set the model to evaluation mode\n",
                "model.eval()\n",
                "validation_loss = 0.0\n",
                "\n",
                "with torch.no_grad():\n",
                "  \n",
                "  for data in validationloader:\n",
                "    \n",
                "      outputs = model(data[0])\n",
                "      loss = criterion(outputs, data[1])\n",
                "      \n",
                "      # Sum the current loss to the validation_loss variable\n",
                "      validation_loss += loss.item()\n",
                "\n",
                "# Set the model to evaluation mode\n",
                "model.eval()\n",
                "validation_loss = 0.0\n",
                "\n",
                "with torch.no_grad():\n",
                "  \n",
                "  for data in validationloader:\n",
                "    \n",
                "      outputs = model(data[0])\n",
                "      loss = criterion(outputs, data[1])\n",
                "      \n",
                "      # Sum the current loss to the validation_loss variable\n",
                "      validation_loss += loss.item()\n",
                "      \n",
                "# Calculate the mean loss value\n",
                "validation_loss_epoch = validation_loss / len(validationloader)\n",
                "print(validation_loss_epoch)\n",
                "\n",
                "# Set the model back to training mode\n",
                "model.train()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Calculating accuracy using torchmetrics\n",
                "\n",
                "In addition to the losses, you should also be keeping track of the\n",
                "accuracy during training. By doing so, you will be able to select the\n",
                "epoch when the model performed the best.\n",
                "\n",
                "In this exercise, you will practice using the `torchmetrics` package to\n",
                "calculate the accuracy. You will be using a sample of the facemask\n",
                "dataset. This dataset contains three different classes. The\n",
                "`plot_errors` function will display samples where the model predictions\n",
                "do not match the ground truth. Performing such error analysis will help\n",
                "you understand your model failure modes.\n",
                "\n",
                "The `torchmetrics` package is already imported. The model `outputs` are\n",
                "the probabilities returned by a softmax as the last step of the model.\n",
                "The `labels` tensor contains the labels as one-hot encoded vectors.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create an accuracy metric for a `\"multiclass\"` problem with three\n",
                "  classes.\n",
                "- Calculate the accuracy for each batch of the dataloader.\n",
                "- Calculate accuracy for the epoch.\n",
                "- Reset the metric for the next epoch.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create accuracy metric using torch metrics\n",
                "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n",
                "for data in dataloader:\n",
                "    features, labels = data\n",
                "    outputs = model(features)\n",
                "    \n",
                "    # Calculate accuracy over the batch\n",
                "    acc = metric(outputs, labels.argmax(dim=-1))\n",
                "\n",
                "# Create accuracy metric using torch metrics\n",
                "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n",
                "for data in dataloader:\n",
                "    features, labels = data\n",
                "    outputs = model(features)\n",
                "    \n",
                "    # Calculate accuracy over the batch\n",
                "    acc = metric(outputs.softmax(dim=-1), labels.argmax(dim=-1))\n",
                "    \n",
                "# Calculate accuracy over the whole epoch\n",
                "acc = metric.compute()\n",
                "\n",
                "# Reset the metric for the next epoch (training or validation)\n",
                "metric.reset()\n",
                "plot_errors(model, dataloader)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Experimenting with dropout\n",
                "\n",
                "The dropout layer randomly zeroes out elements of the input tensor.\n",
                "Doing so helps fight overfitting. In this exercise, you'll create a\n",
                "small neural network with at least two linear layers, two dropout\n",
                "layers, and two activation functions.\n",
                "\n",
                "The `torch.nn` package has already been imported as `nn`. An\n",
                "`input_tensor` of dimensions $1 \\times 3072$ has been created for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create a small neural network with one linear layer, one ReLU\n",
                "  function, and one dropout layer, in that order.\n",
                "- The model should take `input_tensor` as input and return an output of\n",
                "  size 16.\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "- Using the same neural network, set the probability of zeroing out\n",
                "  elements in the dropout layer to `0.8`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a small neural network\n",
                "model = nn.Sequential(nn.Linear(3072, 16),\n",
                "                      nn.ReLU(),\n",
                "                      nn.Dropout())\n",
                "model(input_tensor)\n",
                "\n",
                "# Using the same model, set the dropout probability to 0.8\n",
                "model = nn.Sequential(nn.Linear(3072, 16),\n",
                "                      nn.ReLU(),\n",
                "                      nn.Dropout(p=0.8))\n",
                "model(input_tensor)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Implementing random search\n",
                "\n",
                "Hyperparameter search is a computationally costly approach to experiment\n",
                "with different hyperparameter values. However, it can lead to\n",
                "performance improvements. In this exercise, you will implement a random\n",
                "search algorithm.\n",
                "\n",
                "You will randomly sample 10 values of the learning rate and momentum\n",
                "from the uniform distribution. To do so, you will use the\n",
                "`np.random.uniform()` function.\n",
                "\n",
                "The `numpy` package has already been imported as `np`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Randomly sample a learning rate factor between `2` and `4` so that the\n",
                "  learning rate (`lr`) is bounded between $10^{-2}$ and $10^{-4}$.\n",
                "- Randomly sample a momentum between 0.85 and 0.99.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "values = []\n",
                "for idx in range(10):\n",
                "    # Randomly sample a learning rate factor 2 and 4\n",
                "    factor = np.random.uniform(2, 4)\n",
                "    lr = 10 ** -factor\n",
                "    \n",
                "    # Randomly sample a momentum between 0.85 and 0.99\n",
                "    momentum = np.random.uniform(0.85, 0.99)\n",
                "    \n",
                "    values.append((lr, momentum))\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
